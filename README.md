# transformer
[The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention)와 [pytorch 공식문서](https://tutorials.pytorch.kr/beginner/transformer_tutorial.html)를 
참고하여 트랜스포머를 직접 개발해보고 한글-영어 번역 태스크에 대해 학습 및 테스트.
![]('./images/transformer-translation.png')
## Data
### ① AI Hub 한국어-영어 번역 샘플 데이터.
샘플 데이터의 경우 회원 가입 및 로그인 할 필요 없음.
- url: https://aihub.or.kr/sample_data_board
### ② AI Hub 한국어-영어 번역 데이터 - 구어체
사용허가를 받아 
## Model

## Train
```text
model
  transformer.py    ﹒﹒﹒ 트랜스포머 모델
  util.py           ﹒﹒﹒ 모델에 사용되는 유틸
  visualization.py  ﹒﹒﹒ 모델의 시각화에 사용하는 
```

### Train Result
```
-----------------------------------------------------------------------------------------
| end of epoch   0 | time: 2005.31s | valid loss  4.95 | valid ppl   141.70
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 2149.59s | valid loss  4.62 | valid ppl   101.26
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 2058.49s | valid loss  4.39 | valid ppl    80.86
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 1966.75s | valid loss  4.25 | valid ppl    70.38
-----------------------------------------------------------------------------------------
| end of epoch   4 | time: 1964.57s | valid loss  4.17 | valid ppl    64.82
-----------------------------------------------------------------------------------------
| end of epoch   5 | time: 1933.66s | valid loss  4.02 | valid ppl    55.44
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 1979.88s | valid loss  3.93 | valid ppl    51.11
-----------------------------------------------------------------------------------------
| end of epoch   7 | time: 1996.13s | valid loss  3.86 | valid ppl    47.37
-----------------------------------------------------------------------------------------
| end of epoch   8 | time: 1957.73s | valid loss  3.79 | valid ppl    44.17
-----------------------------------------------------------------------------------------
| end of epoch   9 | time: 1967.79s | valid loss  3.69 | valid ppl    40.17
-----------------------------------------------------------------------------------------
| end of epoch  10 | time: 1993.58s | valid loss  3.62 | valid ppl    37.41
-----------------------------------------------------------------------------------------
| end of epoch  11 | time: 1980.43s | valid loss  3.57 | valid ppl    35.45
-----------------------------------------------------------------------------------------
| end of epoch  12 | time: 1992.14s | valid loss  3.50 | valid ppl    33.11
-----------------------------------------------------------------------------------------
| end of epoch  13 | time: 1976.84s | valid loss  3.46 | valid ppl    31.92
-----------------------------------------------------------------------------------------
```


## Issue
### 1. encoder 마스킹 에러
mask에 unsqueeze(1)을 통해 하나의 차원을 추가해줘야한다. 
```py
  mask = mask.unsqueeze(1)
  attention_score = attention_score.masked_fill(mask == 0, -1e9)
```
### 2. max_seq_len이 긴경우
대부분의 문장이 짧게 구성되어 있다. 최초에 512 토근으로 지정후 학습하면, 시간도 느리고, pad 토큰에 대해 학습하여
성능이 좋지 않았다.


## References
- http://nlp.seas.harvard.edu/2018/04/03/attention